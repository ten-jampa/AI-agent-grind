{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd2f9366",
   "metadata": {},
   "source": [
    "# Before proceeding into the Agent building part, Let's focus on Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f966c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from google import genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from load_env import gemini_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e505c023",
   "metadata": {},
   "source": [
    "### Instantiating a Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fab24dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model = 'gemini-2.0-flash',\n",
    "    api_key = gemini_api_key,\n",
    "    temperature = 0.3,\n",
    "    max_tokens = 1500,\n",
    "    timeout = None,\n",
    "    max_retries = 2,\n",
    "    # some other parameters\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b030a",
   "metadata": {},
   "source": [
    "### Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffc99739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='我喜欢编程 (Wǒ xǐhuan biānchéng)', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--e9f64fac-2d91-4bce-b27e-793b6a4bde28-0', usage_metadata={'input_tokens': 18, 'output_tokens': 14, 'total_tokens': 32, 'input_token_details': {'cache_read': 0}})\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful assistant that translates English to Chinese. Translate the user sentence\"),\n",
    "    (\"user\", \"I love programming\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1105c957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'我喜欢编程 (Wǒ xǐhuan biānchéng)'\n"
     ]
    }
   ],
   "source": [
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd84c038",
   "metadata": {},
   "source": [
    "### Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff021fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ich liebe Programmieren.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--e5e8267d-c415-4560-85e7-195d29fdde82-0', usage_metadata={'input_tokens': 14, 'output_tokens': 7, 'total_tokens': 21, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use ChatPromptTemplate to create a prompt for the model\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            'system',\n",
    "            \"\"\"You are a helpful assistant that translates {input_language} to {output_language}.\"\"\",\n",
    "        ),\n",
    "        ('human', '{input}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm # what is this doing?\n",
    "\n",
    "chain.invoke({\n",
    "    'input_language': 'English',\n",
    "    'output_language': 'German',\n",
    "    'input': 'I love programming'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c6274",
   "metadata": {},
   "source": [
    "## Multimodal Usage\n",
    "\n",
    "Gemini models can accept multimodal inputs (text, images, audio, and video) and \n",
    ", for some models, generate multimodal outputs. We will concern ourselves with that in a different notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0495f97",
   "metadata": {},
   "source": [
    "## Tool Calling\n",
    "\n",
    "You can equip the model with tools to use in its responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3787ff77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'args': {'location': 'Tokyo'},\n",
      "  'id': 'd0b8bbaf-be4e-4ae5-a99f-56559ed6fed2',\n",
      "  'name': 'get_weather',\n",
      "  'type': 'tool_call'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zl/fqyfw3z95bjccqn0m2sm7qhw0000gn/T/ipykernel_42240/2081460996.py:25: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  content = get_weather(*ai_response.tool_calls[0]['args']),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='OK. The weather in Tokyo is sunny.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--d46d7fdc-fcf7-4c77-88ae-9e4a4130ba7a-0', usage_metadata={'input_tokens': 31, 'output_tokens': 10, 'total_tokens': 41, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Define the tool\n",
    "@tool(description=\"Get the current weather in a given location\")\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    return f\"The weather in {location} is sunny\"\n",
    "\n",
    "# Initialize the model and bind the tool\n",
    "llm = ChatGoogleGenerativeAI(model = 'gemini-2.0-flash', api_key = gemini_api_key)\n",
    "llm_with_tools = llm.bind_tools([get_weather])\n",
    "\n",
    "# Invoke the model with a query that should trigger the tool\n",
    "\n",
    "query = 'What is the weather in Tokyo?'\n",
    "ai_response = llm_with_tools.invoke(query)\n",
    "\n",
    "# check the tool calls in the response\n",
    "pprint(ai_response.tool_calls) # list of tools and relevant arguments\n",
    "\n",
    "# Example tool call message would be needed here if you were actually using the tool\n",
    "from langchain_core.messages import ToolMessage\n",
    "tool_message = ToolMessage(\n",
    "    content = get_weather(*ai_response.tool_calls[0]['args']),\n",
    "    tool_call_id = ai_response.tool_calls[0]['id']\n",
    ")\n",
    "\n",
    "llm_with_tools.invoke([ai_response,tool_message]) # example of passingtool result back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7263c462",
   "metadata": {},
   "source": [
    "## Structured Output\n",
    "\n",
    "Force the model to respond with a specific structure using Pydantic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62bb8875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Abraham Lincoln' height_m=1.93\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Define the desired structure\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person\"\"\"\n",
    "\n",
    "    name: str = Field(...,description=\"The name of the person\")\n",
    "    height_m: float = Field(...,description=\"The height of the person in meters\")\n",
    "\n",
    "# Initialize the model\n",
    "llm = ChatGoogleGenerativeAI(model = 'gemini-2.0-flash', api_key = gemini_api_key, temperature=0)\n",
    "structured_llm = llm.with_structured_output(Person) # this is where you enforce a response\n",
    "\n",
    "# Invoke the model\n",
    "query = \"Who was the 16th president of the USA and how tall was he?\"\n",
    "response = structured_llm.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea24574e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='John Doe', height_m=1.8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the model again\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "query = \"What is the weather in Tokyo?\"\n",
    "response = structured_llm.invoke(query)\n",
    "response # so I am guessing that we get a default response if the model does not know the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1413c172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Minato Namikaze', height_m=1.75)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invoke the model again\n",
    "query = 'Who is the fourth Hokage of the leaf village and how tall was he?'\n",
    "response = structured_llm.invoke(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056ebe74",
   "metadata": {},
   "source": [
    "## Token Usage Tracking\n",
    "\n",
    "Access token usage information from the response metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85632dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering is the art and science of crafting effective text inputs to guide large language models toward generating desired outputs.\n",
      "\n",
      "Usage Metadata:\n",
      "{'input_tokens': 10, 'output_tokens': 23, 'total_tokens': 33, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model = 'gemini-2.0-flash', api_key = gemini_api_key)\n",
    "\n",
    "query = 'Explain the concept of prompt engineering in one sentence.'\n",
    "result = llm.invoke(query)\n",
    "\n",
    "print(result.content)\n",
    "print('\\nUsage Metadata:')\n",
    "print(result.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fab0a8",
   "metadata": {},
   "source": [
    "## Built-in Tools\n",
    "\n",
    "Google Gemini supports a varieyt of built-in tools (googlesearch, code execution), which can be bound to the model in the usual way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9eae4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The most recent total solar eclipse in the US was on April 8, 2024. The next '\n",
      " 'total solar eclipse in the US will occur on March 30, 2033, but it will only '\n",
      " 'be visible in Alaska. The next total solar eclipse visible in the contiguous '\n",
      " 'United States will be on August 22, 2044. The path of totality will only be '\n",
      " 'visible in Montana, North Dakota, and South Dakota. Another total solar '\n",
      " 'eclipse will occur on August 12, 2045, and will be visible from California '\n",
      " 'to Florida.')\n"
     ]
    }
   ],
   "source": [
    "from google.ai.generativelanguage_v1beta import Tool as GenAI_Tool\n",
    "\n",
    "resp = llm.invoke(\n",
    "    'When is the next total solar eclipse in the US and when was the last one?',\n",
    "    tools = [GenAI_Tool(google_search={})],\n",
    ")\n",
    "\n",
    "pprint(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b77bd584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tenzinjampa/Downloads/personal_projects/AI-agent-grind/venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:686: UserWarning: \n",
      "        ⚠️ Warning: Output may vary each run.  \n",
      "        - 'executable_code': Always present.  \n",
      "        - 'execution_result' & 'image_url': May be absent for some queries.  \n",
      "\n",
      "        Validate before using in production.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=[{'type': 'executable_code', 'executable_code': 'print(2*2)\\n', 'language': <Language.PYTHON: 1>}, {'type': 'code_execution_result', 'code_execution_result': '4\\n', 'outcome': <Outcome.OUTCOME_OK: 1>}, '2 * 2 = 4'], additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--f4bcef38-1889-4989-9158-f40d8b4e995a-0', usage_metadata={'input_tokens': 9, 'output_tokens': 17, 'total_tokens': 44, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.ai.generativelanguage_v1beta.types import Tool as GenAITool\n",
    "\n",
    "resp = llm.invoke(\n",
    "    \"What is 2*2, use python\",\n",
    "    tools=[GenAITool(code_execution={})],\n",
    ")\n",
    "\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b12b912d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executable code: print(2*2)\n",
      "\n",
      "Code execution result: 4\n",
      "\n",
      "2 * 2 = 4\n"
     ]
    }
   ],
   "source": [
    "for c in resp.content:\n",
    "    if isinstance(c, dict):\n",
    "        if c[\"type\"] == \"code_execution_result\":\n",
    "            print(f\"Code execution result: {c['code_execution_result']}\")\n",
    "        elif c[\"type\"] == \"executable_code\":\n",
    "            print(f\"Executable code: {c['executable_code']}\")\n",
    "    else:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a7dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
